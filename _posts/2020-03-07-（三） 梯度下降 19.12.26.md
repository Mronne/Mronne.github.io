---
layout: article
title: （三）梯度下降
tag: [机器学习课程,gradient descent,deep learning]
excerpt: 机器学习中的梯度下降优化求解
---

求解如下优化问题
$$
\theta^*=\arg\min_{\theta}L(\theta)
$$

# 调整学习率 $\eta$
$$
\theta^i=\theta^{i-1}-\eta\nabla L(\theta^{i-1})
$$
绘制损失函数关参数的曲线以进行调整
![image](http://m.qpic.cn/psc?/V10GdCbE4Hg3EY/Kl*GVNe9OdIAJBN6RDL7pKCGzyZHfzVWP7FxIlpo29lir4i9xGxfA0PDLxVaLZhk6X.GHeoXGEObvdM.Ek10pEfQNa9p8CvvxHztUHU3sPI!/b&bo=jAURBAAAAAADB74!&rf=viewer_4)

## 1.自适应调整学习率
- 在迭代初期，距离目标值较远，可设置较大的学习率
- 经过迭代后，距离目标值较近，可减小学习率
- 对不同的参数应用不同的学习率 $\eta$

### Adagrad
$$
w^{t+1}\gets w^t-\frac{\eta^t}{\sigma^t}g^t\\[0.4em]
\eta^t=\frac{\eta}{\sqrt{t+1}} \quad g^t=\frac{\partial L(\theta^t)}{\partial w}
$$
其中 $\sigma^t$ 代表过去所有梯度值的平方和均值的平方根
$$
\sigma^t=\sqrt{\frac{1}{t+1}\sum_{i=0}^t(g^i)^2}
$$
化简得
$$
w^{t+1}\gets w^t-\frac{\eta}{\sqrt{\sum_{i=0}^t(g^i)^2}}g^t
$$
只能适用于单一参数

## 2.随机梯度下降（Stochastic Gradient Descent）
$$
L=\sum_{n}\bigg(\hat{y}^n-(b+\sum w_ix_i^n)\bigg)^2
$$
1. 随机选择一个 $x^n$
2. 计算当前位置的误差并更新
$$
L^n=\bigg(\hat{y}^n-(b+\sum w_ix_i^n)\bigg)^2\\[0.5em]

\theta^i=\theta^{i-1}-\eta\nabla L^n(\theta^{i-1})
$$
- 迭代速度很快

## 3.特征缩放 （Feature Scaling）
将输入变量缩放为统一规模，可有效提高求解速度和效果
![image](http://m.qpic.cn/psc?/V10GdCbE4Hg3EY/Kl*GVNe9OdIAJBN6RDL7pPxcfWqaGky.t0BTKXlezPQJBnSy7bRgthL99N2t9puTTmGOWh4klSSmP9jWoosy5aIGQUcvIDmdlYCpwhDNoUo!/b&bo=iwXPAwAAAAARB3I!&rf=viewer_4)

- 求解每个输入向量 $x^i$ 的均值 $m_i$ 及标准差 $\sigma_i$
- 对向量中的每个元素执行操作
$$
x_i^r\gets \frac{x_i^r-m_i}{\sigma_i}
$$
- 经过操作后，输入向量的均值都为0，方差都为1
