![image](http://m.qpic.cn/psc?/V10GdCbE4Hg3EY/Kl*GVNe9OdIAJBN6RDL7pJcks5awUQrbDX5MnOIHR1zELG8POh*ttzCPK55qZSjIUMOJtZyczCcoB9Wv61KCJECPMmZSEEQPP1pZUOdAVIE!/b&bo=pgPDAgAAAAARB1Q!&rf=viewer_4)

[toc]

# 1.训练集上表现不佳

##  梯度爆炸问题
- 多层神经网络在靠近输入层的地方梯度较小，在靠近输出层的地方梯度较大。
- 这会导致在求导时，输入层变化步长很小，输出层变化步长很大。
- 当输入层还处于随机变化状态时，输出层已经收敛。出现了梯度爆炸问题
- 
## 1.1 调整网络架构
## 1.2 设置新的激活函数
### 1.2.1 Sigmod函数的不足
- 对于Sigmod函数来说，输入变化很大，输出变化很小
- 如果网络中的激活函数都是Sigmod函数，输入层的参数经过多层Sigmod函数衰减后，输入层的变化对输出几乎没有影响

### 1.2.2 ReLU函数 Rectified Linear Unit
![image](http://m.qpic.cn/psc?/V10GdCbE4Hg3EY/Kl*GVNe9OdIAJBN6RDL7pFA9rdhozjTJaZJ.LpdKuVYPRDc3PPIjvu72R7PFZyhvnFIjV5sKaWLTZVg0A9ZOevem2M1Eud.iEbb4wdLew1E!/b&bo=UAXpAgAAAAARB44!&rf=viewer_4)
- ReLU函数的输出要么是0，要么是线性的
- 对于输出为0的神经元，其对输出层没有影响
- 剩下的输出为线性的神经元，假如输入有变化，输出可响应其变化
- 对于ReLU的非线性：
    - 假如输入变化很小，ReLU为线性
    - 假如输入变换很大，ReLU的作用范围由线性区域变到了其他区域，即非线性的
- 不可微问题 跳过z=0的点 小于0微分为0  大于0微分为1

![image](http://m.qpic.cn/psc?/V10GdCbE4Hg3EY/Kl*GVNe9OdIAJBN6RDL7pMaoK85y3nbJ96DlzpGESSFgrwUWVqpzCfJdTk*CyvSXSW0H1D13nTwcK8jLKUeloOqErR7ofDXX4Bojtv16HIk!/b&bo=IwWmAwAAAAADB6E!&rf=viewer_4)
### 1.2.3 Maxout
- 自动学习激活函数
![image](http://m.qpic.cn/psc?/V10GdCbE4Hg3EY/Kl*GVNe9OdIAJBN6RDL7pAomQKEZ9VfCCIG2a074SCOfCZW4NRpfeNyyCmXhCxVBXb*IJPGVbvLAEhWH0xwhjhHLT5sOhSpRE6TYvEZ1owg!/b&bo=aQW5AwAAAAADN8Q!&rf=viewer_4)

![image](http://m.qpic.cn/psc?/V10GdCbE4Hg3EY/Kl*GVNe9OdIAJBN6RDL7pG89ei*WhBGtFJNOXH8DOC4.7HLhNe027y3pogdBlqGQeFN1t0iiDQpJhR.1H1bmb9BAH.LRDsSFcrR5N1kkhF8!/b&bo=bgUeAwAAAAARF1Y!&rf=viewer_4)

- 训练技巧
    - 只有每个比较组中最大的元素对输出有影响
    - 去除其他对输出无影响的值，模型变成了一个线性模型
    - 给定不同的输入值，每组选的最大值不一样，网络结构会发生改变，随着输入样本的增多，可以使网络中的参数都得到训练

## 1.3 改进梯度求解算法
### 1.3.1 Adagrad不足
```math
w^{t+1}\gets w^t-\frac{\eta}{\sqrt{\sum_{i=0}^t(g^i)^2}}g^t
```
- 对于梯度较大的区域会计算得到较小的学习率
- 对于梯度较小的区域会计算得到较大的学习率

神经网络中的优化函数有时非常复杂，学习率调整需要更加迅速

![image](http://m.qpic.cn/psc?/V10GdCbE4Hg3EY/Kl*GVNe9OdIAJBN6RDL7pL4oHsTp2XN9X3VXuFBN00B85AbpIKp9501z96We2cK9bEBfLXzEGKgG23x6Q3Az8l9xx4*Forb9GqVlkn5L8e0!/b&bo=GgXfAgAAAAARB*I!&rf=viewer_4)
### 1.3.2 RMSProp（改进的Adagrad）  
- 可以通过调整`$\alpha$`使学习率更加考虑新求得的梯度

![image](http://m.qpic.cn/psc?/V10GdCbE4Hg3EY/Kl*GVNe9OdIAJBN6RDL7pN.4H*gvQJiJVubbak0m65pPVzO2ZxMh6CihExgj*XbQVBaxAfAQ7b7.zSXp531.JX7ggNHsyfBwQFYge0s0Ank!/b&bo=ZAUoAwAAAAADN1g!&rf=viewer_4)

### 1.3.3 Momenteum 梯度下降
- 针对有时出现局部最小值或者导数为0导致无法进行求解的问题，提出了Momenteum方法
- 现在的梯度加上前一个时间移动的方向（相当于考虑了惯性）
![image](http://m.qpic.cn/psc?/V10GdCbE4Hg3EY/Kl*GVNe9OdIAJBN6RDL7pD6WtNc6pkeAgF2nv5Zv8Dg61NLpe96LzqOGg0XFjuDm7hitdQH3JzcR4oq0BQQVH0vnZ77q1Aaw0smbTvnyzO0!/b&bo=cQVNAwAAAAADBxg!&rf=viewer_4 )

### 1.3.4 Adam （Momenteum+RMSProp）
![image](http://m.qpic.cn/psc?/V10GdCbE4Hg3EY/Kl*GVNe9OdIAJBN6RDL7pK3RmXVTJgtTzlo0n8n46wKuCTabZhMJocXL2DQWK*VHTLdujWdoMopMIhSp8iMk2BkyQN.PhGhmAsAW*DMv7wE!/b&bo=RQXPAwAAAAARB7w!&rf=viewer_4)
# 2.测试集(valid set)上表现不佳
上述测试集指的是自己分割出的validation set
## 2.1 Early Stopping 提前停止
![image](http://m.qpic.cn/psc?/V10GdCbE4Hg3EY/Kl*GVNe9OdIAJBN6RDL7pFwif*Abpjyr3yy3x2Lsg3Dvy0Ra6sFzuRKWMXfrGG2IY1dWNuGLkHKFFtT9TYFR3S9F1n9aYc38PGkswhJTdb4!/b&bo=nQS6AgAAAAARBxE!&rf=viewer_4)

## 2.2 Regularization 正则化
重新定义Loss Function

- L2 范数正则化项
```math
L'(\theta)=L(\theta)+\lambda \frac{1}{2}\|\theta\|_2\\[0.4em]
\|\theta\|_2=(w_1)^2+(w_2)^2+...\\[0.6em]
\frac{\partial L'}{\partial w}=\frac{\partial L}{\partial w}+\lambda w\\[0.4em]
Update：w^{t+1}\gets w^t - \eta \frac{\partial L'}{\partial w}\\[0.5em]
=(1-\eta \lambda)w^t-\eta \frac{\partial L}{\partial w}
```

- L1 范数正则化项
```math
L'(\theta)=L(\theta)+\lambda \frac{1}{2}\|\theta\|_1\\[0.4em]
\|\theta\|_1=|w_1|+|w_2|+...\\[0.6em]
\frac{\partial L'}{\partial w}=\frac{\partial L}{\partial w}+\lambda sgn(w)\\[0.4em]
Update：w^{t+1}\gets w^t - \eta \frac{\partial L'}{\partial w}\\[0.5em]
=w^t-\eta \frac{\partial L}{\partial w}-\eta \lambda sgn(w^t)
```

- 实际都是改变原有的参数
    - L1范数产生的结果是Sparse的，有一些很大的值，有一些很小的值
    - L2范数产生的结果是平均都比较小

## 2.3 Dropout
- 训练时
    - 在每次更新参数前每个神经元有`$p\%$`的概率被丢掉（dropout）
    - 此时网络结构会发生改变，使用新的网络进行训练
    - 会导致在training set上的结果变差，但会在testing set上的结果变好
- test时
    - 不做dropout
    - 所有的权重乘上`$(1-p\%)$`
- 原因
    - 训练时负重训练，测试时重拳出击


![image](http://m.qpic.cn/psc?/V10GdCbE4Hg3EY/Kl*GVNe9OdIAJBN6RDL7pIdrwrHNVfogoiqf7kucEqdM0fDGz60.C0VtlwfFWxkXnQ3yNBj7h1YsP5tEoYbFmmeymFeowwAVmdLAukg9ARk!/b&bo=bQVCAwAAAAADBws!&rf=viewer_4)

![image](http://m.qpic.cn/psc?/V10GdCbE4Hg3EY/Kl*GVNe9OdIAJBN6RDL7pN.xnObnncrRbrJC63nJe2dxb0LQam6KJVNOFY*EeURq5bZ5zVuc27jaDVZyue73FATBD1khP.LDzOZPxPSNC2E!/b&bo=iwUoAwAAAAADB4c!&rf=viewer_4)